"use strict";(self.webpackChunkbasc_website=self.webpackChunkbasc_website||[]).push([[477],{10:e=>{e.exports=JSON.parse('{"blogPosts":[{"id":"taiwan","metadata":{"permalink":"/blog/taiwan","source":"@site/blog/2023-12-06-taiwan.md","title":"Evaluating Taiwan\'s Tactics to Safeguard its Semiconductor Assets Against a Chinese Invasion","description":"In the following report, we examine the strategies Taiwan may employ to protect both itself and its semiconductor industry.","date":"2023-12-06T00:00:00.000Z","formattedDate":"December 6, 2023","tags":[{"label":"Governance","permalink":"/blog/tags/governance"}],"readingTime":1.35,"hasTruncateMarker":true,"authors":[{"name":"Gaurav Yadav","title":"Policy Research Lead & Co-Founder","url":"https://bristolaisafety.org/about#gaurav","imageURL":"https://media.licdn.com/dms/image/D4D03AQGRWSI_qmKhUg/profile-displayphoto-shrink_800_800/0/1667520413208?e=1703116800&v=beta&t=yRnZPHZAF3yPHy6GOxwSLic-zSGMPL1Yix93bkDuze0","key":"gaurav"}],"frontMatter":{"slug":"taiwan","title":"Evaluating Taiwan\'s Tactics to Safeguard its Semiconductor Assets Against a Chinese Invasion","authors":["gaurav"],"tags":["Governance"]},"nextItem":{"title":"STARC: A General Framework For Quantifying Differences Between Reward Functions","permalink":"/blog/starc"}},"content":"In the following report, we examine the strategies Taiwan may employ to protect both itself and its semiconductor industry. \\n\\nWe discuss four strategies: 1) Scorch the Fabs - Taiwan threatens or executes the destruction of their fabs to deter invasion or deny their use for China - we conclude that the credibility of this approach is limited and intractable; 2) Sabotage the Fabs - leveraging vulnerabilities in the semiconductor manufacturing process, Taiwan subtly impairs key fabrication equipment - this strategy is suggested to be a last-resort, we consider this approach a more credible alternative to demolition; 3) Boatlift Key Staff - boatlifting personnel possessing tacit knowledge could deny China control of the fabs, the success of this strategy hinges upon the accuracy of the evacuating party to identify irreplaceable personnel and the willingness of said personnel to return if needed - given this logistical challenge, we consider this course of action unlikely, and; 4) Indirect Protection of Semiconductor Assets - while it is probable that sanctions would be imposed following an invasion, and that these sanctions would be economically damaging for all involved, we argue that it is unlikely that such efforts could be effectively maintained. Following this thought, we argue that while an invasion would violate international law, this is likely to be disregarded and unlikely to prevent Chinese aggression.\xa0\\n\\nOur report represents an attempt to map out the consequences of a Chinese invasion of Taiwan, a topic that appears to be neglected despite its clear significance to the future of AI research.\\n\\n\x3c!--truncate--\x3e\\n\\nThis was authored by Gaurav Yadav and Robert Reason.\\n\\nYou can read the full paper\xa0[here](../docs/taiwan.pdf)."},{"id":"starc","metadata":{"permalink":"/blog/starc","source":"@site/blog/2023-09-26-starc.md","title":"STARC: A General Framework For Quantifying Differences Between Reward Functions","description":"In order to solve a task using reinforcement learning, it is necessary to first formalise the goal of that task as a reward function. However, for many real-world tasks, it is very difficult to manually specify a reward function that never incentivises undesirable behaviour. As a result, it is increasingly popular to use reward learning algorithms, which attempt to learn a reward function from data. However, the theoretical foundations of reward learning are not yet well-developed. In particular, it is typically not known when a given reward learning algorithm with high probability will learn a reward function that is safe to optimise. This means that reward learning algorithms generally must be evaluated empirically, which is expensive, and that their failure modes are difficult to predict in advance. One of the roadblocks to deriving better theoretical guarantees is the lack of good methods for quantifying the difference between reward functions. In this paper we provide a solution to this problem, in the form of a class of pseudometrics on the space of all reward functions that we call STARC (STAndardised Reward Comparison) metrics. We show that STARC metrics induce both an upper and a lower bound on worst-case regret, which implies that our metrics are tight, and that any metric with the same properties must be bilipschitz equivalent to ours. Moreover, we also identify a number of issues with reward metrics proposed by earlier works. Finally, we evaluate our metrics empirically, to demonstrate their practical efficacy. STARC metrics can be used to make both theoretical and empirical analysis of reward learning algorithms both easier and more principled.","date":"2023-09-26T00:00:00.000Z","formattedDate":"September 26, 2023","tags":[{"label":"Safe RL","permalink":"/blog/tags/safe-rl"}],"readingTime":1.46,"hasTruncateMarker":true,"authors":[{"name":"Lucy Farnik","title":"Technical Research Lead & Co-Founder","url":"https://www.linkedin.com/in/lucy-farnik/","imageURL":"https://media.licdn.com/dms/image/D4E03AQFjPP8Z2krXgw/profile-displayphoto-shrink_800_800/0/1675185850759?e=1701907200&v=beta&t=gkbOKuWdeQ1eureEkn9CoptxxXWRWeROkV83JlCFWBI","key":"lucy"}],"frontMatter":{"slug":"starc","title":"STARC: A General Framework For Quantifying Differences Between Reward Functions","authors":["lucy"],"tags":["Safe RL"]},"prevItem":{"title":"Evaluating Taiwan\'s Tactics to Safeguard its Semiconductor Assets Against a Chinese Invasion","permalink":"/blog/taiwan"},"nextItem":{"title":"Sparse Autoencoders Find Highly Interpretable Features in Language Models","permalink":"/blog/autoencoders"}},"content":"In order to solve a task using reinforcement learning, it is necessary to first formalise the goal of that task as a reward function. However, for many real-world tasks, it is very difficult to manually specify a reward function that never incentivises undesirable behaviour. As a result, it is increasingly popular to use reward learning algorithms, which attempt to learn a reward function from data. However, the theoretical foundations of reward learning are not yet well-developed. In particular, it is typically not known when a given reward learning algorithm with high probability will learn a reward function that is safe to optimise. This means that reward learning algorithms generally must be evaluated empirically, which is expensive, and that their failure modes are difficult to predict in advance. One of the roadblocks to deriving better theoretical guarantees is the lack of good methods for quantifying the difference between reward functions. In this paper we provide a solution to this problem, in the form of a class of pseudometrics on the space of all reward functions that we call STARC (STAndardised Reward Comparison) metrics. We show that STARC metrics induce both an upper and a lower bound on worst-case regret, which implies that our metrics are tight, and that any metric with the same properties must be bilipschitz equivalent to ours. Moreover, we also identify a number of issues with reward metrics proposed by earlier works. Finally, we evaluate our metrics empirically, to demonstrate their practical efficacy. STARC metrics can be used to make both theoretical and empirical analysis of reward learning algorithms both easier and more principled.\\n\\n\x3c!--truncate--\x3e\\n\\nThis was authored by Joar Skalse, Lucy Farnik (BASC), Sumeet Ramesh Motwani, Erik Jenner, Adam Gleave, Alessandro Abate.\\n\\nYou can read the full paper [here](https://arxiv.org/abs/2309.15257)."},{"id":"autoencoders","metadata":{"permalink":"/blog/autoencoders","source":"@site/blog/2023-09-19-autoencoders.md","title":"Sparse Autoencoders Find Highly Interpretable Features in Language Models","description":"One of the roadblocks to a better understanding of neural networks\u2019 internals is polysemanticity, where neurons appear to activate in multiple, semantically distinct contexts. Polysemanticity prevents us from identifying concise, human understandable explanations for what neural networks are doing internally. One hypothesised cause of polysemanticity is superposition, where neural networks represent more features than they have neurons by assigning features to an overcomplete set of directions in activation space, rather than to individual neurons. Here, we attempt to identify those directions, using sparse autoencoders to reconstruct the internal activations of a language model. These autoencoders learn sets of sparsely activating features that are more interpretable and monosemantic than directions identified by alternative approaches, where interpretability is measured by automated methods. Moreover, we show that with our learned set of features, we can pinpoint the features that are causally responsible for counterfactual behaviour on the indirect object identification task (Wang et al., 2022) to a finer degree than previous decompositions. This work indicates that it is possible to resolve superposition in language models using a scalable, unsupervised method. Our method may serve as a foundation for future mechanistic interpretability work, which we hope will enable greater model transparency and steerability.","date":"2023-09-19T00:00:00.000Z","formattedDate":"September 19, 2023","tags":[{"label":"Interpretability","permalink":"/blog/tags/interpretability"}],"readingTime":1.115,"hasTruncateMarker":true,"authors":[{"name":"Aidan Ewert","title":"Technical Research Lead & Co-Founder","url":"https://www.linkedin.com/in/aidan-ewart-648a30204/","imageURL":"https://media.licdn.com/dms/image/D4E03AQH7DsgUq4aFWg/profile-displayphoto-shrink_800_800/0/1697468770646?e=1703116800&v=beta&t=IUcocG6Yog7VcEBJeviXrs80tr0sUSyMeLePtM8Z7lo","key":"aidan"}],"frontMatter":{"slug":"autoencoders","title":"Sparse Autoencoders Find Highly Interpretable Features in Language Models","authors":["aidan"],"tags":["Interpretability"]},"prevItem":{"title":"STARC: A General Framework For Quantifying Differences Between Reward Functions","permalink":"/blog/starc"},"nextItem":{"title":"How long will reaching a Risk Awareness Moment and CHARTS agreement take?","permalink":"/blog/risk-awareness-moment"}},"content":"One of the roadblocks to a better understanding of neural networks\u2019 internals is polysemanticity, where neurons appear to activate in multiple, semantically distinct contexts. Polysemanticity prevents us from identifying concise, human understandable explanations for what neural networks are doing internally. One hypothesised cause of polysemanticity is superposition, where neural networks represent more features than they have neurons by assigning features to an overcomplete set of directions in activation space, rather than to individual neurons. Here, we attempt to identify those directions, using sparse autoencoders to reconstruct the internal activations of a language model. These autoencoders learn sets of sparsely activating features that are more interpretable and monosemantic than directions identified by alternative approaches, where interpretability is measured by automated methods. Moreover, we show that with our learned set of features, we can pinpoint the features that are causally responsible for counterfactual behaviour on the indirect object identification task (Wang et al., 2022) to a finer degree than previous decompositions. This work indicates that it is possible to resolve superposition in language models using a scalable, unsupervised method. Our method may serve as a foundation for future mechanistic interpretability work, which we hope will enable greater model transparency and steerability.\\n\\n\x3c!--truncate--\x3e\\n\\nThis was authored by Hoagy Cunningham, Aidan Ewart (BASC), Logan Riggs, Robert Huben and Lee Sharkey.\\n\\nYou can read the full paper [here](https://arxiv.org/abs/2309.08600)."},{"id":"risk-awareness-moment","metadata":{"permalink":"/blog/risk-awareness-moment","source":"@site/blog/2023-09-06-risk-awareness.md","title":"How long will reaching a Risk Awareness Moment and CHARTS agreement take?","description":"This report aims to develop a forecast to an open question from the analysis, \u2018Prospects for AI safety agreements between countries\u2019 (Guest, 2023): Is there sufficient time to have a \u2018risk awareness moment\u2019 in either the US (along with its allies) and China in place before an international AI safety agreement can no longer meaningfully reduce extinction risks from AI?","date":"2023-09-06T00:00:00.000Z","formattedDate":"September 6, 2023","tags":[{"label":"Governance","permalink":"/blog/tags/governance"}],"readingTime":0.5,"hasTruncateMarker":true,"authors":[{"name":"Gaurav Yadav","title":"Policy Research Lead & Co-Founder","url":"https://bristolaisafety.org/about#gaurav","imageURL":"https://media.licdn.com/dms/image/D4D03AQGRWSI_qmKhUg/profile-displayphoto-shrink_800_800/0/1667520413208?e=1703116800&v=beta&t=yRnZPHZAF3yPHy6GOxwSLic-zSGMPL1Yix93bkDuze0","key":"gaurav"}],"frontMatter":{"slug":"risk-awareness-moment","title":"How long will reaching a Risk Awareness Moment and CHARTS agreement take?","authors":["gaurav"],"tags":["Governance"]},"prevItem":{"title":"Sparse Autoencoders Find Highly Interpretable Features in Language Models","permalink":"/blog/autoencoders"}},"content":"This report aims to develop a forecast to an open question from the analysis, \u2018Prospects for AI safety agreements between countries\u2019 (Guest, 2023): Is there sufficient time to have a \u2018risk awareness moment\u2019 in either the US (along with its allies) and China in place before an international AI safety agreement can no longer meaningfully reduce extinction risks from AI?\\n\\nBottom line: My overall estimate/best guess is that there is at least a 40% chance there will be adequate time to implement a CHARTS agreement before it ceases to be relevant.\\n\\n\x3c!--truncate--\x3e\\n\\nYou can read the report in full [here](https://docs.google.com/document/d/1MLmzULVrksH8IwAmH7wBpOTyU_7BjBAVl6N63v2vvhM/edit)."}]}')}}]);